# EventPulse NC Development Guide

## ğŸš€ Getting Started

### Prerequisites
- Node.js 18+ 
- Python 3.8+
- npm or yarn
- Git

### Initial Setup

1. **Clone and setup the project**
   ```bash
   git clone <repository-url>
   cd eventpulse-nc
   ```

2. **Install all dependencies**
   ```bash
   # Backend dependencies
   cd backend && npm install
   
   # Frontend dependencies  
   cd ../frontend && npm install
   
   # Python dependencies
   cd ../scraper && pip install -r requirements.txt
   ```

3. **Start the development environment**
   ```bash
   # Terminal 1: Start backend
   cd backend && npm run dev
   
   # Terminal 2: Start frontend
   cd frontend && npm start
   
   # Terminal 3: Run scrapers (when needed)
   cd scraper && python main.py
   ```

## ğŸ—ï¸ Architecture Overview

### Backend (Node.js/Express)
- **Port**: 5050
- **Database**: SQLite (events.db)
- **Key Files**:
  - `index.js` - Main server file
  - `db.js` - Database configuration
  - `package.json` - Dependencies and scripts

### Frontend (React/TypeScript)
- **Port**: 3000
- **Framework**: React 18 with TypeScript
- **Styling**: Tailwind CSS
- **Key Files**:
  - `src/App.tsx` - Main application
  - `src/components/` - React components
  - `src/services/api.ts` - API communication
  - `src/types/Event.ts` - TypeScript types

### Scrapers (Python)
- **Framework**: Custom scraper classes
- **Key Files**:
  - `main.py` - Scraper orchestration
  - `base_scraper.py` - Base scraper class
  - `*_scraper.py` - Individual scrapers

## ğŸ“ Project Structure

```
eventpulse-nc/
â”œâ”€â”€ backend/                 # Node.js API server
â”‚   â”œâ”€â”€ index.js            # Express server
â”‚   â”œâ”€â”€ db.js               # Database setup
â”‚   â”œâ”€â”€ events.db           # SQLite database
â”‚   â””â”€â”€ package.json        # Backend dependencies
â”œâ”€â”€ frontend/               # React application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/     # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ EventList.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ EventDetail.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ EventMap.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Navbar.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ LoadingSpinner.tsx
â”‚   â”‚   â”‚   â””â”€â”€ ErrorBoundary.tsx
â”‚   â”‚   â”œâ”€â”€ services/       # API services
â”‚   â”‚   â”‚   â””â”€â”€ api.ts
â”‚   â”‚   â”œâ”€â”€ types/          # TypeScript types
â”‚   â”‚   â”‚   â””â”€â”€ Event.ts
â”‚   â”‚   â””â”€â”€ App.tsx         # Main application
â”‚   â””â”€â”€ package.json        # Frontend dependencies
â”œâ”€â”€ scraper/                # Python scraping engine
â”‚   â”œâ”€â”€ main.py             # Main scraper orchestration
â”‚   â”œâ”€â”€ base_scraper.py     # Base scraper class
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â””â”€â”€ *_scraper.py        # Individual scrapers
â””â”€â”€ docs/                   # Documentation
    â””â”€â”€ DEVELOPMENT.md      # This file
```

## ğŸ”§ Development Workflow

### Adding New Features

1. **Create a feature branch**
   ```bash
   git checkout -b feature/new-feature-name
   ```

2. **Make your changes**
   - Follow the coding standards below
   - Add tests for new functionality
   - Update documentation

3. **Test your changes**
   ```bash
   # Test backend
   cd backend && npm test
   
   # Test frontend
   cd frontend && npm test
   
   # Test scrapers
   cd scraper && python -m pytest
   ```

4. **Submit a pull request**
   - Include a clear description
   - Reference any related issues
   - Request code review

### Adding New Scrapers

1. **Create a new scraper file**
   ```python
   # scraper/new_source_scraper.py
   from base_scraper import BaseScraper
   from post_event import post_event
   
   class NewSourceScraper(BaseScraper):
       def __init__(self):
           super().__init__("New Source", "https://example.com")
       
       def parse_events(self, html):
           # Implement parsing logic
           pass
   ```

2. **Add to main.py**
   ```python
   from new_source_scraper import NewSourceScraper
   
   # In main section
   print("ğŸ” New Source Events")
   NewSourceScraper().run_and_post()
   ```

3. **Test the scraper**
   ```bash
   cd scraper && python -c "from new_source_scraper import NewSourceScraper; NewSourceScraper().run_and_post()"
   ```

## ğŸ¨ Coding Standards

### JavaScript/TypeScript
- Use ES6+ features
- Prefer const/let over var
- Use meaningful variable names
- Add JSDoc comments for functions
- Follow Airbnb style guide

### Python
- Follow PEP 8
- Use meaningful variable names
- Add docstrings to classes and functions
- Use type hints where possible

### React Components
- Use functional components with hooks
- Keep components small and focused
- Use TypeScript interfaces for props
- Follow the component naming convention

## ğŸ§ª Testing

### Backend Testing
```bash
cd backend
npm test
```

### Frontend Testing
```bash
cd frontend
npm test
```

### Scraper Testing
```bash
cd scraper
python -m pytest tests/
```

## ğŸš€ Deployment

### Production Build
```bash
# Frontend
cd frontend && npm run build

# Backend
cd backend && npm start
```

### Environment Variables
Create `.env` files for each environment:

```bash
# Backend .env
PORT=5050
NODE_ENV=production
FRONTEND_URL=https://yourdomain.com

# Frontend .env
REACT_APP_API_URL=https://api.yourdomain.com
```

## ğŸ” Debugging

### Backend Debugging
```bash
# Enable debug logging
DEBUG=* npm run dev

# Check database
sqlite3 backend/events.db "SELECT * FROM events LIMIT 5;"
```

### Frontend Debugging
- Use React Developer Tools
- Check browser console
- Use Redux DevTools (if using Redux)

### Scraper Debugging
```bash
# Run individual scraper with verbose output
cd scraper && python -c "from ncsu_scraper import NCSUScraper; scraper = NCSUScraper(); print(scraper.fetch_html(scraper.base_url)[:500])"
```

## ğŸ“Š Performance Optimization

### Backend
- Implement caching (Redis)
- Add database indexes
- Use connection pooling
- Implement rate limiting

### Frontend
- Code splitting with React.lazy()
- Optimize bundle size
- Use React.memo() for expensive components
- Implement virtual scrolling for large lists

### Scrapers
- Add retry logic
- Implement concurrent scraping
- Cache scraped data
- Add rate limiting

## ğŸ”’ Security

### Backend Security
- Input validation
- SQL injection prevention
- CORS configuration
- Rate limiting
- Helmet.js for headers

### Frontend Security
- XSS prevention
- Content Security Policy
- HTTPS enforcement
- Secure cookie handling

## ğŸ“ˆ Monitoring

### Logging
- Use structured logging
- Log levels (error, warn, info, debug)
- Centralized log management

### Metrics
- API response times
- Error rates
- Database performance
- Scraper success rates

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Update documentation
6. Submit a pull request

## ğŸ†˜ Troubleshooting

### Common Issues

**Backend won't start**
- Check if port 5050 is available
- Verify database permissions
- Check Node.js version

**Frontend won't start**
- Check if port 3000 is available
- Clear npm cache: `npm cache clean --force`
- Delete node_modules and reinstall

**Scrapers failing**
- Check internet connection
- Verify target websites are accessible
- Check for rate limiting
- Update user agents if needed

**Database issues**
- Check SQLite file permissions
- Verify database schema
- Backup and recreate if corrupted

## ğŸ“š Additional Resources

- [React Documentation](https://reactjs.org/docs/)
- [Express.js Guide](https://expressjs.com/en/guide/routing.html)
- [SQLite Documentation](https://www.sqlite.org/docs.html)
- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/)
- [Tailwind CSS Documentation](https://tailwindcss.com/docs) 